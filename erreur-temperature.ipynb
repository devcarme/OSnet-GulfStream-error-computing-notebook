{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Demo how to make predictions with OSnet, using all built-in default variables\n",
    "\n",
    "|<img src=\"https://github.com/euroargodev/OSnet-GulfStream/raw/main/docs/_static/osnet_landscape.png\" width=\"300px\"/>|\n",
    "|:---------:|\n",
    "|``OSnet`` is a python library to make T/S/MLD predictions in the Gulf Stream Extension using Neural Network|\n",
    "\n",
    "<b>Warning: These predictions are made using climatological input fields for SST and SLA !</b>\n",
    "\n",
    "This notebook requires cartopy, cmocean and argopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr><p>This notebook has been developed at the Laboratory for Ocean Physics and Satellite remote sensing, Ifremer,     within the framework of the Euro-ArgoRISE project. <br>This project has received funding from the European     Unionâ€™s Horizon 2020 research and innovation programme under grant agreement no 824131. Call INFRADEV-03-2018-2019:     Individual support to ESFRI and other world-class research infrastructures.</p><a href='https://www.euro-argo.eu/EU-Projects/Euro-Argo-RISE-2019-2022'><img src='https://raw.githubusercontent.com/euroargodev/OSnet-GulfStream/main/docs/_static/logo_earise.png?token=GHSAT0AAAAAABQFXZESQOABJYKRM5KODBWWYRCB4BQ' style='height:75px'></a><a href='https://www.umr-lops.fr'><img src='https://raw.githubusercontent.com/euroargodev/OSnet-GulfStream/main/docs/_static/logo_lops.jpg?token=GHSAT0AAAAAABQFXZET7WPE545OZTBNDMEQYRCB4JQ' style='height:75px'></a><a href='https://wwz.ifremer.fr'><img src='https://raw.githubusercontent.com/euroargodev/OSnet-GulfStream/main/docs/_static/logo_ifremer.jpg?token=GHSAT0AAAAAABQFXZESH5XSCWHO4ZLLDUAAYRCB4PQ' style='height:75px'></a><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Assume OSnet is one parent folder away from this notebook:\n",
    "sys.path.insert(0, os.path.split(os.path.abspath(os.path.curdir))[0])  \n",
    "import osnet\n",
    "from osnet.options import OPTIONS\n",
    "osnet.disclaimer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import argopy\n",
    "\n",
    "# For plots\n",
    "xr.set_options(display_style=\"text\", display_expand_attrs=False);\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import colorcet as cc\n",
    "import cmocean\n",
    "import time\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "\n",
    "# Dictionnary of colormaps for variables\n",
    "cmapd = {'BATHY': cmocean.cm.topo, \n",
    "      'MDT': cmocean.cm.delta, \n",
    "      'SST': cmocean.cm.thermal, \n",
    "      'SLA': cmocean.cm.balance, \n",
    "      'UGOSA': cmocean.cm.balance, \n",
    "      'VGOSA': cmocean.cm.balance, \n",
    "      'UGOS': cmocean.cm.balance,\n",
    "      'VGOS': cmocean.cm.balance,\n",
    "      'temp': cmocean.cm.thermal, \n",
    "      'temp_adj': cmocean.cm.thermal, \n",
    "      'temp_std': cmocean.cm.amp, \n",
    "      'psal': cmocean.cm.haline, \n",
    "      'psal_adj': cmocean.cm.haline, \n",
    "      'psal_std': cmocean.cm.amp, \n",
    "      'sig': cmocean.cm.dense, \n",
    "      'sig_adj': cmocean.cm.dense, \n",
    "      'sig_std': cmocean.cm.amp, \n",
    "      'mld': cc.cm[\"rainbow\"]\n",
    "     }\n",
    "\n",
    "\n",
    "def add_map(this_ax):\n",
    "    \"\"\"\n",
    "        >>> fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,6), dpi=90, subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "        >>> add_map(ax)\n",
    "    \"\"\"\n",
    "    this_ax.set_xlim(osnet.utilities.conv_lon(OPTIONS['domain'][0])-1, osnet.utilities.conv_lon(OPTIONS['domain'][1])+1)\n",
    "    this_ax.set_ylim(OPTIONS['domain'][2]-1, OPTIONS['domain'][3]+1)\n",
    "    #Display lands\n",
    "    this_ax.coastlines()\n",
    "    #Display grid\n",
    "    gl = this_ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "    \n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    gl.xlines = False\n",
    "    #Display lat long in margin\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER\n",
    "    \n",
    "    return this_ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load OSnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loick/miniconda3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.4 s, sys: 110 ms, total: 4.51 s\n",
      "Wall time: 4.71 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<osnet.GulfStream>\n",
       "Reference: Pauthenet et al, 2022 (http://dx.doi.org/...)\n",
       "Models: 15 instance(s) in the ensemble\n",
       "MLD adjusted: True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "m = osnet.load('Gulf-Stream')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a collection of input grid to make prediction for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Horizontal Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:                (N_PROF: 23, PRES_INTERPOLATED: 51)\n",
      "Coordinates:\n",
      "  * N_PROF                 (N_PROF) int64 58 77 70 57 50 46 ... 66 85 64 1 69 42\n",
      "    LATITUDE               (N_PROF) float64 43.46 24.17 24.03 ... 26.79 31.07\n",
      "    LONGITUDE              (N_PROF) float64 -31.68 -32.07 ... -53.72 -62.08\n",
      "    TIME                   (N_PROF) datetime64[ns] 2020-08-01T05:57:23 ... 20...\n",
      "  * PRES_INTERPOLATED      (PRES_INTERPOLATED) int64 0 1 2 3 ... 857 950 1000\n",
      "Data variables:\n",
      "    CONFIG_MISSION_NUMBER  (N_PROF) int32 4 -1 8 5 1 1 7 1 ... 4 4 4 -1 4 5 4 6\n",
      "    CYCLE_NUMBER           (N_PROF) int32 106 284 66 125 67 ... 65 54 222 56 222\n",
      "    DATA_MODE              (N_PROF) <U1 'D' 'A' 'D' 'D' 'D' ... 'D' 'D' 'D' 'D'\n",
      "    DIRECTION              (N_PROF) <U1 'A' 'A' 'A' 'A' 'A' ... 'A' 'A' 'A' 'A'\n",
      "    PLATFORM_NUMBER        (N_PROF) int32 4902919 6901146 ... 4903056 4902354\n",
      "    PRES                   (N_PROF, PRES_INTERPOLATED) float32 1.08 ... 1e+03\n",
      "    PSAL                   (N_PROF, PRES_INTERPOLATED) float32 35.77 ... 35.14\n",
      "    TEMP                   (N_PROF, PRES_INTERPOLATED) float32 21.53 ... 6.802\n",
      "Attributes: (8)\n"
     ]
    }
   ],
   "source": [
    "from argopy import DataFetcher as ArgoDataFetcher\n",
    "argo_loader = ArgoDataFetcher()\n",
    "\n",
    "#small region\n",
    "#ds = argo_loader.region([-55,-54,35,36,0,1000, '2010-08-23','2010-08-24']).to_xarray()\n",
    "#ds = argo_loader.region([-74,-70,30,35,0,1000, '2010-08-01','2010-08-31']).to_xarray()\n",
    "\n",
    "#large region\n",
    "#ds = argo_loader.region([-74,-64,30,40,0,1000, '2010-08-01','2010-08-31']).to_xarray()\n",
    "\n",
    "#GulfStream region 2005\n",
    "#ds = argo_loader.region([-81.0,-29.5,22.0,50.5,0,1000, '2005-07-01','2005-08-07']).to_xarray()\n",
    "\n",
    "#GulfStream region 2010\n",
    "#ds = argo_loader.region([-81.0,-29.5,22.0,50.5,0,1000, '2010-08-02','2010-08-08']).to_xarray()\n",
    "#ds = argo_loader.region([-81.0,-29.5,22.0,50.5,0,1000, '2010-08-15','2010-08-25']).to_xarray()\n",
    "\n",
    "#GulfStream region 2015\n",
    "#ds = argo_loader.region([-81.0,-29.5,22.0,50.5,0,1000, '2015-08-02','2015-08-06']).to_xarray()\n",
    "\n",
    "#GulfStream region 2018\n",
    "#ds = argo_loader.region([-81.0,-29.5,22.0,50.5,0,1000, '2018-08-02','2018-08-09']).to_xarray()\n",
    "\n",
    "#GulfStream region 2020\n",
    "ds = argo_loader.region([-81.0,-29.5,22.0,50.5,0,1000, '2020-08-01','2020-08-08']).to_xarray()\n",
    "\n",
    "#Transform a collection of points into a collection of profiles\n",
    "ds_profile = ds.argo.point2profile()\n",
    "\n",
    "#51 standards level of pressure from the prediction model\n",
    "standard_levels = m.SDL\n",
    "\n",
    "#Interpolate measurements to standard pressure levels\n",
    "ds_interp = ds_profile.argo.interp_std_levels(standard_levels)\n",
    "print(ds_interp)\n",
    "\n",
    "#Array of temperatures\n",
    "temp_data = [i for i in ds_interp['TEMP'].data.flatten()]\n",
    "\n",
    "\n",
    "#DataSet for predictions\n",
    "ds_in = xr.Dataset({}, coords={\n",
    "    'lon': (['lon'], ds_interp['LONGITUDE'].data),\n",
    "    'lat': (['lat'], ds_interp['LATITUDE'].data),\n",
    "    'time': (['time'], ds_interp['TIME'].data),\n",
    "    'N_PROF': (['N_PROF'], ds_interp['N_PROF'].data)\n",
    "})\n",
    "\n",
    "input_list['snapshot'] = ds_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meridional transect (with one longitude)\n",
    "ds_in = xr.Dataset({}, coords={\n",
    "    'lon': (['lon'], [ds_interp['LONGITUDE'].data[(int((len(ds_interp['LONGITUDE'].data)-1)/2))]]),\n",
    "    'lat': (['lat'], ds_interp['LATITUDE'].data),\n",
    "    'time': ds_interp['TIME'].data,\n",
    "    'N_PROF': (['N_PROF'], ds_interp['N_PROF'].data)\n",
    "})\n",
    "\n",
    "input_list['meridional_transect'] = ds_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zonal transect (with one latitude)\n",
    "\n",
    "ds_in = xr.Dataset({}, coords={\n",
    "    'lon': (['lon'], ds_interp['LONGITUDE'].data),\n",
    "    'lat': (['lat'], [ds_interp['LATITUDE'].data[(int((len(ds_interp['LONGITUDE'].data)-1)/2))]]),\n",
    "    'time': ds_interp['TIME'].data,\n",
    "    'N_PROF': (['N_PROF'], ds_interp['N_PROF'].data)\n",
    "})\n",
    "\n",
    "input_list['zonal_transect'] = ds_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions for all inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions for: snapshot\n",
      "8266/8266 [==============================] - 17s 2ms/step\n",
      "8266/8266 [==============================] - 16s 2ms/step\n",
      "8266/8266 [==============================] - 15s 2ms/step\n",
      "8266/8266 [==============================] - 13s 2ms/step\n",
      "8266/8266 [==============================] - 13s 2ms/step\n",
      "8266/8266 [==============================] - 14s 2ms/step\n",
      "8266/8266 [==============================] - 11s 1ms/step\n",
      "8266/8266 [==============================] - 17s 2ms/step\n",
      "8266/8266 [==============================] - 16s 2ms/step\n",
      "8266/8266 [==============================] - 15s 2ms/step\n",
      "8266/8266 [==============================] - 16s 2ms/step\n",
      "8266/8266 [==============================] - 17s 2ms/step\n",
      "8266/8266 [==============================] - 19s 2ms/step\n",
      "8266/8266 [==============================] - 14s 2ms/step\n",
      "8266/8266 [==============================] - 15s 2ms/step\n",
      "\tdone in 332.814 secs (1.258 ms/N, with N=264500)\n",
      "Make predictions for: meridional_transect\n",
      "381/381 [==============================] - 1s 2ms/step\n",
      "381/381 [==============================] - 1s 2ms/step\n",
      "381/381 [==============================] - 1s 2ms/step\n",
      "381/381 [==============================] - 1s 4ms/step\n",
      "381/381 [==============================] - 1s 3ms/step\n",
      "381/381 [==============================] - 1s 2ms/step\n",
      "381/381 [==============================] - 1s 2ms/step\n",
      "381/381 [==============================] - 1s 2ms/step\n",
      "381/381 [==============================] - 1s 2ms/step\n",
      "381/381 [==============================] - 1s 2ms/step\n",
      "381/381 [==============================] - 1s 2ms/step\n",
      "381/381 [==============================] - 1s 2ms/step\n",
      "381/381 [==============================] - 1s 2ms/step\n",
      "381/381 [==============================] - 1s 2ms/step\n",
      "381/381 [==============================] - 1s 2ms/step\n",
      "\tdone in 19.969 secs (1.641 ms/N, with N=12167)\n",
      "Make predictions for: zonal_transect\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      " 70/348 [=====>........................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "output_list = {}\n",
    "for key, ds_in in input_list.items():\n",
    "    print(\"Make predictions for: %s\" % key)\n",
    "    start = time.time()\n",
    "    output_list[key] = m.predict(ds_in)\n",
    "    Nsample = output_list[key].attrs['OSnet-Nsample']\n",
    "    dt = time.time() - start\n",
    "    print(\"\\tdone in %0.3f secs (%0.3f ms/N, with N=%i)\" % (dt, dt/Nsample*1e3, Nsample))\n",
    "    # # ds_out = m.predict(ds_in, inplace=False)\n",
    "    # # ds_out = m.predict(ds_in, keep_added=True)\n",
    "    # # ds_out = m.predict(ds_in, keep_added=True, adjust_mld=False)\n",
    "    # # ds_out = m.predict(ds_in, scaled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out = output_list['snapshot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load an existing dataset from a netCDF file\n",
    "#ds_disk = xr.open_dataset(\"error_computing_from2010-08-01to2010-08-04.nc\")\n",
    "#print(ds_disk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min and max argo temperature data\n",
    "min_temp_data = min(temp_data)\n",
    "max_temp_data = max(temp_data)\n",
    "\n",
    "#Start time\n",
    "start_error_computing = time.time()\n",
    "\n",
    "#New variables relative_error in the xarray\n",
    "ds_out = ds_out.assign(relative_error=ds_out.temp_adj)\n",
    "\n",
    "#Profiles\n",
    "for i in range(len(ds_out['N_PROF'])):\n",
    "    #Pressure levels\n",
    "    for j in range(len(ds_out['PRES_INTERPOLATED'])):\n",
    "        #Relative error computing\n",
    "        ds_out['temp_adj'][j][0][0][0][i] = (abs(ds_out['temp'][j][0][0][0][i] - ds_interp['TEMP'][i][j])) / (max_temp_data - min_temp_data)\n",
    "        ds_out['relative_error'][j][0][0][0][i] = (abs(ds_out['temp'][j][0][0][0][i] - ds_interp['TEMP'][i][j])) / (max_temp_data - min_temp_data) \n",
    "        \n",
    "#Endtime\n",
    "dt_error_computing = time.time() - start_error_computing\n",
    "print(\"\\terror computing done in %0.3f secs (%0.3f ms/N, with N=%i)\" % (dt_error_computing, dt_error_computing/Nsample*1e3, Nsample))\n",
    "\n",
    "\n",
    "#Save in netCdf file\n",
    "ds_out.to_netcdf(\"error_computing_from%sto%s.nc\" % (pd.to_datetime(ds_out['time'].values[0]).strftime('%Y-%m-%d'),pd.to_datetime(ds_out['time'].values[len(ds_out['time'].values)-1]).strftime('%Y-%m-%d')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2D horizontal snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out = output_list['snapshot']\n",
    "\n",
    "#DataSet without N_PROF for the visualization \n",
    "ds_out_no_nprof = ds_out.drop_vars('N_PROF')\n",
    "ds_out_no_nprof = ds_out_no_nprof.isel(N_PROF=0)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=1, nrows=2, figsize=(20,10), sharex=True, sharey=True, subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "ax = np.array(ax).flatten()\n",
    "vlist = ['temp_adj']\n",
    "    \n",
    "for i, v in enumerate([(z,v) for z in [0,400] for v in vlist]):\n",
    "    z, vname = v[0], v[1]\n",
    "    if 'PRES_INTERPOLATED' in ds_out[vname].dims:\n",
    "        this = ds_out_no_nprof[vname].isel(time=0).sel(PRES_INTERPOLATED=z, method='nearest')\n",
    "    else:\n",
    "        this = ds_out_no_nprof[vname].isel(time=0)\n",
    "\n",
    "    add_map(ax[i])\n",
    "    cs = this.plot.contourf(x='lon', ax=ax[i], levels=12, cmap=cmapd[vname], cbar_kwargs={'orientation':'horizontal', 'label':'relative_error in %'})\n",
    "    ax[i].set_xlabel('')\n",
    "    ax[i].set_ylabel('')\n",
    "    ax[i].set_title(\"%s for z=%0.2f\" % ('error', z))\n",
    "fig.suptitle('OSnet error prediction of a horizontal snapshot\\nfrom %s to %s' % (pd.to_datetime(ds_out['time'].values[0]).strftime('%Y-%m-%d'),pd.to_datetime(ds_out['time'].values[len(ds_out['time'])-1]).strftime('%Y-%m-%d')));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coupe pour une longitute donnÃ©e\n",
    "ds_out_one_lon = output_list['meridional_transect']\n",
    "\n",
    "#New variable relative_error in the xarray\n",
    "ds_out_one_lon = ds_out_one_lon.assign(relative_error=ds_out_one_lon.temp_adj)\n",
    "\n",
    "#Profiles\n",
    "for i in range(len(ds_out_one_lon['N_PROF'])):\n",
    "    #Pressure levels\n",
    "    for j in range(len(ds_out_one_lon['PRES_INTERPOLATED'])):\n",
    "        #Error computing\n",
    "        ds_out_one_lon['temp_adj'][j][0][0][0][i] = (abs(ds_out_one_lon['temp'][j][0][0][0][i] - ds_interp['TEMP'][i][j])) / (max_temp_data - min_temp_data)\n",
    "        ds_out_one_lon['relative_error'][j][0][0][0][i] = (abs(ds_out_one_lon['temp'][j][0][0][0][i] - ds_interp['TEMP'][i][j])) / (max_temp_data - min_temp_data) \n",
    "\n",
    "#DataSet without N_PROF for the visualization \n",
    "ds_out_one_lon = ds_out_one_lon.drop_vars('N_PROF')\n",
    "ds_out_one_lon = ds_out_one_lon.isel(N_PROF=0)\n",
    "\n",
    "        \n",
    "if 'temp_adj' in ds_out_one_lon:\n",
    "    fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(20,7), dpi=180, sharex=True, sharey=True)\n",
    "    ax = np.array(ax).flatten()\n",
    "    vlist = ['temp_adj']\n",
    "else:\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(20,7), dpi=180, sharex=True, sharey=True)\n",
    "    ax = np.array(ax).flatten()\n",
    "    vlist = ['temp_adj']\n",
    "    \n",
    "for i, v in enumerate(vlist):\n",
    "    this = ds_out_one_lon[v].isel(time=0, lon=0)\n",
    "    this.plot.contourf(x='lat', ax=ax[i], levels=12, cmap=cmapd[v], yincrease=False)\n",
    "    ax[i].plot(ds_out_one_lon['lat'], ds_out_one_lon['mld'].isel(time=0, lon=0), 'r')\n",
    "#     ax[i].set_xlabel('')\n",
    "#     ax[i].set_ylabel('')\n",
    "    ax[i].set_title(\"relative error in %\")\n",
    "fig.suptitle('OSnet prediction of a meridional transect\\nlongitude=%s, from %s to %s' % (ds_out_one_lon['lon'].data, pd.to_datetime(ds_out_one_lon['time'].values[0]).strftime('%Y-%m-%d'), pd.to_datetime(ds_out_one_lon['time'].values[len(ds_out_one_lon['time'])-1]).strftime('%Y-%m-%d')));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coupe pour une latitude donnÃ©e\n",
    "ds_out_coupe_lat = output_list['zonal_transect']\n",
    "\n",
    "#New variable relative_error in the xarray\n",
    "ds_out_coupe_lat = ds_out_coupe_lat.assign(relative_error=ds_out_coupe_lat.temp_adj)\n",
    "\n",
    "for i in range(len(ds_out_coupe_lat['N_PROF'])):\n",
    "    for j in range(len(ds_out_coupe_lat['PRES_INTERPOLATED'])):\n",
    "        ds_out_coupe_lat['temp_adj'][j][0][0][0][i] = (abs(ds_out_coupe_lat['temp'][j][0][0][0][i] - ds_interp['TEMP'][i][j])) / (max_temp_data - min_temp_data)\n",
    "        ds_out_coupe_lat['relative_error'][j][0][0][0][i] = (abs(ds_out_coupe_lat['temp'][j][0][0][0][i] - ds_interp['TEMP'][i][j])) / (max_temp_data - min_temp_data) \n",
    "\n",
    "#DataSet without N_PROF for the visualization\n",
    "ds_out_coupe_lat = ds_out_coupe_lat.drop_vars('N_PROF')\n",
    "ds_out_coupe_lat = ds_out_coupe_lat.isel(N_PROF=0)\n",
    "\n",
    "if 'temp_adj' in ds_out_coupe_lat:\n",
    "    fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(20,7), dpi=180, sharex=True, sharey=True)\n",
    "    ax = np.array(ax).flatten()\n",
    "    vlist = ['temp_adj']\n",
    "else:\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(20,7), dpi=180, sharex=True, sharey=True)\n",
    "    ax = np.array(ax).flatten()\n",
    "    vlist = ['temp_adj']\n",
    "    \n",
    "for i, v in enumerate(vlist):\n",
    "    this = ds_out_coupe_lat[v].isel(time=0, lat=0)\n",
    "    this.plot.contourf(x='lon', ax=ax[i], levels=12, cmap=cmapd[v], yincrease=False)\n",
    "    ax[i].plot(ds_out_coupe_lat['lon'], ds_out_coupe_lat['mld'].isel(time=0, lat=0), 'r')\n",
    "    ax[i].set_title(\"relative error in %\")\n",
    "fig.suptitle('OSnet prediction of a meridional transect\\nlatitude=%s, from %s to %s' % (ds_out_coupe_lat['lat'].data, pd.to_datetime(ds_out_coupe_lat['time'].values[0]).strftime('%Y-%m-%d'), pd.to_datetime(ds_out_one_lon['time'].values[len(ds_out_coupe_lat['time'])-1]).strftime('%Y-%m-%d'))); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osnet.disclaimer()\n",
    "osnet.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OSnet",
   "language": "python",
   "name": "osnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
